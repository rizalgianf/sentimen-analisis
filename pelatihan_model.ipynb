{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51515160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Counts:\n",
      "Sentiment\n",
      "1    11821\n",
      "2     2920\n",
      "0      259\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Timestamp</th>\n",
       "      <th>Username</th>\n",
       "      <th>VideoID</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Cleaned_Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2025-04-07T16:03:13Z</td>\n",
       "      <td>@mochamedadama2567</td>\n",
       "      <td>qlKDYBZysoc</td>\n",
       "      <td>2025, Infinix note 50</td>\n",
       "      <td>2025-04-07T16:03:13Z</td>\n",
       "      <td>infinix note</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-03-29T18:07:25Z</td>\n",
       "      <td>@EkomargoriskiEkomargoriski</td>\n",
       "      <td>qlKDYBZysoc</td>\n",
       "      <td>Setia</td>\n",
       "      <td>2025-03-29T18:07:25Z</td>\n",
       "      <td>setia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-01-29T13:36:05Z</td>\n",
       "      <td>@afdhalaljibran1845</td>\n",
       "      <td>qlKDYBZysoc</td>\n",
       "      <td>pengguna 3 tahun yang lalu hadir nih wkwk awet ...</td>\n",
       "      <td>2025-01-29T13:36:05Z</td>\n",
       "      <td>pengguna  tahun  lalu hadir nih wkwk awet cas k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2024-11-08T09:38:37Z</td>\n",
       "      <td>@belutwakwaw1017</td>\n",
       "      <td>qlKDYBZysoc</td>\n",
       "      <td>Nonton 2024 dulu 3 jutaan skrang dbwah 2 jutaan...</td>\n",
       "      <td>2024-11-08T09:38:37Z</td>\n",
       "      <td>nonton  dulu  jutaan skrang dbwah  jutaan spek ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2024-10-05T09:25:10Z</td>\n",
       "      <td>@donypras6150</td>\n",
       "      <td>qlKDYBZysoc</td>\n",
       "      <td>Mendang mending TECNO</td>\n",
       "      <td>2024-10-05T09:25:10Z</td>\n",
       "      <td>mendang mending tecno</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2022-04-26T22:47:59Z</td>\n",
       "      <td>@muhamadsaiful9570</td>\n",
       "      <td>zp4QcvyvQi0</td>\n",
       "      <td>Bang coba review invinix not 11 nfc</td>\n",
       "      <td>2022-04-26T22:47:59Z</td>\n",
       "      <td>coba review invinix not  nfc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2022-04-26T06:31:35Z</td>\n",
       "      <td>@yusniarzen</td>\n",
       "      <td>zp4QcvyvQi0</td>\n",
       "      <td>Infinix yg ada 2 speker selain ini apa ya</td>\n",
       "      <td>2022-04-26T06:31:35Z</td>\n",
       "      <td>infinix    speker selain  apa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2022-04-26T03:10:40Z</td>\n",
       "      <td>@inxgee</td>\n",
       "      <td>zp4QcvyvQi0</td>\n",
       "      <td>bang coba aktifin DTS audionya, soalnya speaker...</td>\n",
       "      <td>2022-04-26T03:10:40Z</td>\n",
       "      <td>coba aktifin dts audionya soalnya speakernya h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2022-05-14T02:04:47Z</td>\n",
       "      <td>@nitanovilia647</td>\n",
       "      <td>zp4QcvyvQi0</td>\n",
       "      <td>Caranya gimana ya</td>\n",
       "      <td>2022-05-14T02:04:47Z</td>\n",
       "      <td>caranya gimana</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2022-04-26T02:01:05Z</td>\n",
       "      <td>@muhamadfarel183</td>\n",
       "      <td>zp4QcvyvQi0</td>\n",
       "      <td>Itu 2 juta ya bang</td>\n",
       "      <td>2022-04-26T02:01:05Z</td>\n",
       "      <td>juta</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'comments_data.csv'\n",
    "data_new = pd.read_csv(file_path)\n",
    "\n",
    "# Enhanced text preprocessing function\n",
    "def clean_text_enhanced(text):\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags (#hashtag)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    # Hilangkan kata-kata spesifik seperti 'di', 'ke', 'dari', 'yang', 'bang', dan sejenisnya\n",
    "    text = re.sub(r'\\b(di|ke|dari|yang|bang|dan|atau|untuk|dengan|pada|oleh|ini|itu|saja|juga|karena|tetapi|agar|sehingga|adalah|seperti|namun|meskipun|walaupun|bahkan|hanya|masih|sudah|belum|akan|telah|dalam|luar|antara|tanpa|setelah|sebelum|hingga|sampai|sebab|akibat|maka|jadi|kalau|jika|bila|apabila|supaya|meski|walau|serta|atau|dan|pun|lagi|malah|justru|apalagi|bahwa|oleh|untuk|dengan|pada|di|ke|dari|yang|bang|yg|nya|ada|sama|buat|aja|ya)\\b', '', text)\n",
    "    return text\n",
    "\n",
    "# Clean the comment column\n",
    "data_new['Cleaned_Comment'] = data_new['Comment'].apply(clean_text_enhanced)\n",
    "\n",
    "# Enhanced sentiment labeling with more precise keywords\n",
    "def sentiment_label_enhanced(comment):\n",
    "    # Positive sentiment based on specific keywords\n",
    "    positive_keywords = ['baik', 'bagus', 'suka', 'terbaik', 'mantap', 'luar biasa', 'senang', 'puas', 'hebat', 'keren', 'indah', 'menyenangkan', 'asik', 'top', 'cool', 'cakep', 'ciamik', 'murah', 'terjangkau', 'ekonomis', 'worth', 'worth it', 'berharga', 'amazing', 'fantastic', 'superb', 'excellent', 'perfect', 'happy', 'love', 'great', 'awesome', 'laris', 'murah', 'terlaris', 'diskon', 'promo', 'hemat', 'spesial', 'affordable', 'bagus banget', 'sangat puas', 'sangat suka', 'sangat baik', 'sangat keren', 'sangat mantap', 'sangat indah', 'sangat menyenangkan', 'sangat luar biasa', 'sangat worth it', 'sangat murah', 'sangat terjangkau', 'sangat ekonomis', 'sangat hebat', 'sangat amazing', 'sangat fantastic', 'sangat perfect', 'sangat happy', 'sangat love', 'sangat great', 'sangat awesome', 'sangat top', 'sangat cool', 'sangat ciamik', 'sangat spesial', 'sangat affordable', 'sangat worth', 'sangat worth it', 'sangat berharga']\n",
    "    negative_keywords = ['buruk', 'jelek', 'tidak suka', 'gagal', 'tidak puas', 'kekecewaan', 'menyedihkan', 'parah', 'benci', 'sampah', 'payah', 'kecewa', 'zonk', 'hancur', 'nyesek', 'lebay', 'mahal', 'overpriced', 'tidak terjangkau']\n",
    "    # Check for positive keywords\n",
    "    if any(word in comment for word in positive_keywords):\n",
    "        return 2  # Positive\n",
    "    \n",
    "    # Check for negative keywords\n",
    "    elif any(word in comment for word in negative_keywords):\n",
    "        return 0  # Negative\n",
    "    \n",
    "    # Neutral sentiment for all others\n",
    "    else:\n",
    "        return 1  # Neutral\n",
    "\n",
    "# Apply sentiment labels to the 'Cleaned_Comment' column\n",
    "data_new['Sentiment'] = data_new['Cleaned_Comment'].apply(sentiment_label_enhanced)\n",
    "\n",
    "# Display the first few rows of the updated data with cleaned comments and sentiment labels\n",
    "data_new[['Cleaned_Comment', 'Sentiment']].head()\n",
    "\n",
    "# Display the total count of each sentiment label\n",
    "sentiment_counts = data_new['Sentiment'].value_counts()\n",
    "print(\"Sentiment Counts:\")\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Display the DataFrame as a scrollable table\n",
    "display(HTML(data_new.to_html(index=False, max_rows=10, max_cols=7, notebook=True)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba41769",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = data_new['Cleaned_Comment']\n",
    "y = data_new['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization for feature extraction\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33db9cf2",
   "metadata": {},
   "source": [
    "# SVM with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac7752e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9825833333333334, 0.9773333333333334)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# SVM with TF-IDF Vectorizer\n",
    "svm_model = make_pipeline(TfidfVectorizer(max_features=10000), SVC(kernel='linear'))\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy_svm = svm_model.score(X_train, y_train)\n",
    "test_accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "train_accuracy_svm, test_accuracy_svm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5960b5",
   "metadata": {},
   "source": [
    "# Count Vectorizer dan Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a81637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 96.98%\n",
      "Testing Accuracy: 94.30%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Tokenisasi dan pembersihan teks (sudah dilakukan sebelumnya)\n",
    "# Split the data into training and testing sets\n",
    "X = data_new['Cleaned_Comment']\n",
    "y = data_new['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Naive Bayes with CountVectorizer\n",
    "nb_model = make_pipeline(CountVectorizer(max_features=10000), MultinomialNB())\n",
    "\n",
    "# Train the model\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy_nb = nb_model.score(X_train, y_train)\n",
    "test_accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f\"Training Accuracy: {train_accuracy_nb * 100:.2f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_nb * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eafabfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.17%\n",
      "Testing Accuracy: 97.83%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Tokenisasi dan pembersihan teks (sudah dilakukan sebelumnya)\n",
    "# Split the data into training and testing sets\n",
    "X = data_new['Cleaned_Comment']\n",
    "y = data_new['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Bag of Words with Logistic Regression\n",
    "logreg_model = make_pipeline(CountVectorizer(max_features=10000), LogisticRegression(max_iter=1000))\n",
    "\n",
    "# Train the model\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_logreg = logreg_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy_logreg = logreg_model.score(X_train, y_train)\n",
    "test_accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f\"Training Accuracy: {train_accuracy_logreg * 100:.2f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_logreg * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
